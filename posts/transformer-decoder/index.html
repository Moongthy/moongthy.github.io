<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="트랜스포머-디코더" /><meta name="author" content="mng" /><meta property="og:locale" content="en" /><meta name="description" content="영어 “I am good”을 입력하면 프랑스어 “Je vais bien”을 생성하는 번역기를 만든다고 가정하자. 번역기를 만들려면 먼저 입력 문장인 “I am good”을 인코더에 입력해야 한다. 인코더는 입력 문장의 표현을 학습한다. 앞에서 인코더가 입력 문장을 학습하는 방법을 상세히 다뤘다. 이제 이 인코더의 결괏값을 가져와서 디코더에 입력값으로 사용한다. 디코더는 다음 그림과 같이 인코더의 표현을 입력값으로 사용하고 타깃 문장인 “Je vais bein”을 생성한다." /><meta property="og:description" content="영어 “I am good”을 입력하면 프랑스어 “Je vais bien”을 생성하는 번역기를 만든다고 가정하자. 번역기를 만들려면 먼저 입력 문장인 “I am good”을 인코더에 입력해야 한다. 인코더는 입력 문장의 표현을 학습한다. 앞에서 인코더가 입력 문장을 학습하는 방법을 상세히 다뤘다. 이제 이 인코더의 결괏값을 가져와서 디코더에 입력값으로 사용한다. 디코더는 다음 그림과 같이 인코더의 표현을 입력값으로 사용하고 타깃 문장인 “Je vais bein”을 생성한다." /><link rel="canonical" href="https://moongthy.github.io/posts/transformer-decoder/" /><meta property="og:url" content="https://moongthy.github.io/posts/transformer-decoder/" /><meta property="og:site_name" content="MNG" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-09-09T11:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="트랜스포머-디코더" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@mng" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"mng"},"dateModified":"2022-09-09T11:00:00+09:00","datePublished":"2022-09-09T11:00:00+09:00","description":"영어 “I am good”을 입력하면 프랑스어 “Je vais bien”을 생성하는 번역기를 만든다고 가정하자. 번역기를 만들려면 먼저 입력 문장인 “I am good”을 인코더에 입력해야 한다. 인코더는 입력 문장의 표현을 학습한다. 앞에서 인코더가 입력 문장을 학습하는 방법을 상세히 다뤘다. 이제 이 인코더의 결괏값을 가져와서 디코더에 입력값으로 사용한다. 디코더는 다음 그림과 같이 인코더의 표현을 입력값으로 사용하고 타깃 문장인 “Je vais bein”을 생성한다.","headline":"트랜스포머-디코더","mainEntityOfPage":{"@type":"WebPage","@id":"https://moongthy.github.io/posts/transformer-decoder/"},"url":"https://moongthy.github.io/posts/transformer-decoder/"}</script><title>트랜스포머-디코더 | MNG</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="MNG"><meta name="application-name" content="MNG"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/archan.gif" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">MNG</a></div><div class="site-subtitle font-italic">Student, 26</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/Moongthy" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['mungeunj','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>트랜스포머-디코더</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>트랜스포머-디코더</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1662688800" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Sep 9, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> mng </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4472 words"> <em>24 min</em> read</span></div></div></div><div class="post-content"><p>영어 “I am good”을 입력하면 프랑스어 “Je vais bien”을 생성하는 번역기를 만든다고 가정하자. 번역기를 만들려면 먼저 입력 문장인 “I am good”을 인코더에 입력해야 한다. 인코더는 입력 문장의 표현을 학습한다. 앞에서 인코더가 입력 문장을 학습하는 방법을 상세히 다뤘다. 이제 이 인코더의 결괏값을 가져와서 디코더에 입력값으로 사용한다. 디코더는 다음 그림과 같이 인코더의 표현을 입력값으로 사용하고 타깃 문장인 “Je vais bein”을 생성한다.</p><p> <img data-src="/assets/img/transformer/decoder/img1-35.png" alt="" data-proofer-ignore> <em>그림 1-35 트랜스포머의 인코더와 디코더</em></p><p>인코더 부분을 다룰 때 인코더 $N$개를 누적해서 쌓을 수 있다는 것을 배웠다. 인코더와 유사하게 디코더 역시 $N$개를 누적해서 쌓을 수 있다. $N=2$로 예를 들어보자. [그림 1-36]에 표시된 것처럼 하나의 디코더 출력값은 그 위에 있는 디코더의 입력값으로 전송된다. 또한 인코더의 입력 문장 표현 (인코더의 출력값)이 모든 디코더에 전송된다. 즉, 디코더는 이전 디코더의 입력값과 인코더의 표현(인코더의 출력값), 이렇게 2개를 입력 데이터로 받는다.</p><p> <img data-src="/assets/img/transformer/decoder/img1-37.png" alt="" data-proofer-ignore> <em>그림 1-37 시간 스텝 t=1 경우 디코더 예측</em></p><p>시간 스텝 t=2 경우 현재까지의 입력값에 이전 단계 (t-1) 디코더에서 생성한 단어를 추가해 문장의 다음 단어를 생성한다. 즉 [그림 1-38]처럼 디코더는 <code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code>와 “Je”를 입력받아 타깃 문장의 다음 단어를 생성한다.</p><p> <img data-src="/assets/img/transformer/decoder/img1-38.png" alt="" data-proofer-ignore> <em>그림 1-38 시간 스텝 t=2 경우 디코더 예측</em></p><p>위의 방법과 마찬가지로 모든 단계에서 디코더는 이전 단계에서 새로 생성한 단어를 조합해 입력값을 생성하고 다음 단어를 예측하는 방법을 진행한다. 따라서 $t=4$의 경우 <code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code>, “Je”, “vais”, “bien”을 입력하고 다음 단어를 예측한다.</p><p> <img data-src="/assets/img/transformer/decoder/img1-39.png" alt="" data-proofer-ignore> <em>그림 1-39 시간 스텝 t=4 경우 디코더 예측</em></p><p>[그림 1-40]을 통해 알 수있듯이 디코더에서 <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> 토큰을 생성할 때 타깃 문장의 생성이 완료된다.</p><p>인코더의 경우, 입력 문장을 임베딩 행렬로 변환한 후 여기에 위치 인코딩을 더한 값을 입력한다. 마찬가지로 디코더 역시 입력값을 바로 입력하는 것이 아니라 위치 인코딩을 추가한 값을 디코더의 입력값으로 사용한다.</p><p>예를 들어 [그림 1-41]처럼 각 시간 단계의 입력을 임베딩으로 변환한다고 할 때 위치 인코딩 값을 추가한 다음 디코더에 입력한다.</p><p> <img data-src="/assets/img/transformer/decoder/img1-41.png" alt="" data-proofer-ignore> <em>그림 1-41 위치 인코딩이 적용된 인코더와 디코더</em></p><p>하나의 디코더 블록은 다음과 같은 요소들로 구성된다.</p><p> <img data-src="/assets/img/transformer/decoder/img1-42.png" alt="" data-proofer-ignore> <em>그림 1-42 디코더 블록</em></p><p>디코더 블록은 서브레이어 3개로 구성된 인코더 블록과 유사한 구조다.</p><ul><li>마스크된 멀티 헤드 어텐션 (masked multi-head attention)<li>멀티 헤드 어텐션 (multi-head attention)<li>피드포워드 네트워크 (feedforward network)</ul><p>디코더 블록은 인코더 블록과 유사하게 서브레이어에 멀티 헤드 어텐션과 피드포워드 네트워크를 포함한다. 하지만 인코더와 다르게 두 가지 형태의 멀티 헤드 어텐션을 사용한다. 그 중 하나는 어텐션 부분인 마스크된 형태이다.</p><h2 id="131-마스크된-멀티-헤드-어텐션"><span class="mr-2">1.3.1 마스크된 멀티 헤드 어텐션</span><a href="#131-마스크된-멀티-헤드-어텐션" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>영어를 프랑스어로 번역하는 태스크가 있고, 학습 데이터가 다음과 같이 준비되어 있다고 가정하자.</p><p> <img data-src="/assets/img/transformer/decoder/img1-43.png" alt="" data-proofer-ignore> <em>그림 1-43 학습 데이터 예제</em></p><p>위의 데이터를 통해 번역 태스크의 입력과 출력 형태를 이해할 수 있다. 앞에서 번역 모델에 대한 테스트를 수행할 때 디코더에서 타깃 문장을 어떻게 생성하는지 알아봤다.</p><p>모델을 학습할 때는 이미 타깃 문장을 알고 있어서 디코더에 기본으로 타깃 문장 전체를 입력하면 되지만 수정 작업이 조금 필요하다. 디코더에서 문장을 입력할 때 처음에는 <code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code> 토큰을 입력하고 <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code> 토큰이 생성될 때까지 이전 단계에서 예측한 단어를 추가하는 형태로 입력을 반복한다. 따라서 타깃 문장의 시작 부분에 <code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code> 토큰을 추가한 다음 디코더에 입력한다.</p><p>“I am good”을 “Je vais bien”으로 번역한다고 가정해보자. 타깃 문장 시작 부분에 <code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code> 토큰을 추가한 “<code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code> je vais bien”을 디코더에 입력하면 디코더에서 “Je vais bien <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code>”를 출력한다.</p><p> <img data-src="/assets/img/transformer/decoder/img1-44.png" alt="" data-proofer-ignore> <em>그림 1-44 트랜스포머의 인코더, 디코더</em></p><p>그렇다면 세부적으로 어떤 방식으로 작동하는 것인가? 왜 타깃 문장 전체를 입력하고 디코더에서는 한 단계 이동한 형태의 문장을 출력하는 것인가? 이 부분을 좀 더 자세히 알아보자.</p><p>디코더에 입력 문장을 입력할 때 입력 문장을 임베딩 (출력 임베딩 행렬)으로 변환한 후 위치 인코딩을 추가해 디코더에 입력하는 것은 알고 있다. 디코더의 입력 행렬을 $X$라고 하자.</p><p> <img data-src="/assets/img/transformer/decoder/img1-45.png" alt="" data-proofer-ignore> <em>그림 1-45 입력 행렬</em></p><p>행렬 $X$를 디코더에 입력하면 첫 번째 레이어는 마스크된 멀티 헤드 어텐션이 된다. 인코더에서 사용한 멀티 헤드 어텐션과 기본 원리는 같지만 다른 점이 한 가지 있다.</p><p>셀프 어텐션을 구현하면 처음에 $Q, K, V$행렬을 생성한다.멀티 헤드 어텐션을 계산하면 $h$개의 $Q, K, V$행렬을 생성한다. 헤드 $i$의 경우 행렬 $X$에 각각 가중치 행렬 $w_i^Q, w_i^K, w_i^V$를 곱해 $Q_i, K_i, V_i$ 행렬을 얻을 수 있다.</p><p>이제 마스크된 멀티 헤드 어텐션을 살펴보자. 디코더의 입력 문장은 ‘<code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code> Je vais bein”이다. 앞에서 셀프 어텐션은 각 단어의 의미를 이해하기 위해 각 단어와 문장 내 전체 단어를 연결했다. 그런데 디코더에서 문장을 생성할 때 이전 단계에서 생성한 단어만 입력문장으로 넣는다는 점이 중요하다. 예를 들어 $t=2$의 경우 디코더의 입력 단어는 [<code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code>, Je]만 들어간다.즉, 이런 데이터의 특성을 살려 모델 학습을 진행해야 한다. 따라서 셀프 어텐션은 단어와의 연관성을 “Je”만 고려해야 하며, <span style="color: orange">모델이 아직 예측하지 않은 오른쪽의 모든 단어를 마스킹해 학습을 진행한다.</span></p><p> <img data-src="/assets/img/transformer/decoder/img1-46.png" alt="" data-proofer-ignore> <em>그림 1-46 값에 대한 마스킹 처리</em></p><p>이와 같은 단어 마스킹 작업은 셀프 어텐션에서 입력되는 단어에만 집중해 단어를 정확하게 생성하는 긍정적인 효과를 가져온다. 그렇다면 마스킹을 어떻게 수현할 수 있을까? $i$ 헤드의 어텐션 행렬 $Z_i$는 다음 식으로 구할 수 있다.</p>\[Z_i=softmax({QK^T\over\sqrt{d_k}})V_i\]<p>어텐션 행렬을 구하는 첫 번째 단계는 쿼리와 키 행렬 사이의 내적을 계산하는 것이다. [그림 1-48]은 쿼리와 키 행렬 사이의 내적값을 구하고, $\sqrt{d_k}$로 나눈 임의의 결과다.</p><p> <img data-src="/assets/img/transformer/decoder/img1-48.png" alt="" data-proofer-ignore> <em>그림 1-48</em></p><p>위 행렬에 소프트맥스 함수를 적용해 정규화 작업을 수행한다. 소프트맥스 함수를 적용하기 전에 행렬값에 대한 마스킹 처리가 필요하다. 예를 들어 위 행렬의 첫 번째 행을 보자. <code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code>의 다음 단어를 예측한다고 할 때 모델에서는 <code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code> 오른쪽에 있는 모든 단어를 참조하지 말아야한다. <code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code> 오른쪽에 있는 모든 단어를 $-\infty$로 마스킹한다. 두 번째, 세 번째 행도 마찬가지로 수행한다.</p><p> <img data-src="/assets/img/transformer/decoder/score.png" alt="" data-proofer-ignore></p><p>이제 소프트맥스 함수를 적용한 행렬과 밸류 $(V_i)$ 행렬에 곱해 최종적으로 어텐션 행렬 $Z_i$를 구한다. 멀티 헤드 어텐션의 경우 $h$개의 어텐션 행렬을 구하고 이들을 서로 연결한 후에 새로운 가중치 행렬 $W^0$을 곱해 최종적으로 어텐션 행렬 $M$을 구한다.</p>\[M=concatenate(Z_1, Z_2, Z_3, \cdots, Z_h)W_0\]<h2 id="132-멀티-헤드-어텐션"><span class="mr-2">1.3.2 멀티 헤드 어텐션</span><a href="#132-멀티-헤드-어텐션" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[그림 1-52]는 인코더와 디코더를 결합한 트랜스포머 모델의 모습이다. 이때 디코더의 멀티 헤드 어텐션은 입력 데이터 2개를 받는다. 하나는 이전 서브레이어의 출력값이고, 다른 하나는 인코더의 표현이다.</p><p> <img data-src="/assets/img/transformer/decoder/img1-52.png" alt="" data-proofer-ignore> <em>그림 1-52 인코더와 디코더 상호 작용</em></p><p>인코더의 표현 값을 $R$, 이전 서브레이어인 마스크된 멀티 헤드 어텐션의 결과로 나온 어텐션 행렬을 $M$이라고 한다. 여기서 인코더의 결과와 디코더의 결과 사이에 상호 작용이 일어난다. 이를 <span style="color: orange">인코더-디코더 어텐션 레이어 (encoder-decoder attention layer)</span>라고 한다.</p><p>이제 멀티 헤드 어텐션 레이어가 어떻게 작동하는지 알아보자. 첫 번째 단계에서는 멀티 헤드 어텐션에서 사용하는 쿼리, 키, 밸류 행렬을 생성한다. 앞에서 행렬에 가중치 행렬을 곱해서 쿼리, 키, 밸류 행렬을 만들 수 있다는 것을 배웠다. 하지만 이번에는 입력값이 2개 (인코더 표현 $R$, 이전 서브레이어의 결과인 $M$)다. 이런 경우에는 어떻게 해야 할까?</p><p>이전 서브레이어의 출력값인 어텐션 행렬 $M$을 사용해 쿼리 행렬 $Q$를 생성하고, 인코더 표현 값인 $R$을 활용해 $K, V$행렬을 생성한다. 현재 멀티 헤드 어텐션을 사용하고 있으므로 헤드 $i$를 기준으로 다음 절차를 따른다.</p><ul><li>어텐션 행렬 $M$에 가중치 행렬 $W_i^Q$를 곱해 쿼리 행렬 $Q_i$를 생성한다.<li>인코더 표현값 $R$에 가중치 행렬 $W_i^K, W_i^V$를 각각 곱해 키, 밸류 행렬 $K_i, V_i$를 생성한다.</ul><p> <img data-src="/assets/img/transformer/decoder/img1-53.png" alt="" data-proofer-ignore> <em>그림 1-53 쿼리, 키, 밸류 행렬 생성</em></p><p>왜 쿼리 행렬은 $M$을 통해 생성하고 키, 밸류 행렬은 $R$을 통해 생성하는 것일까? 일반적으로 쿼리 행렬은 타깃 문장의 표현을 포함하므로 타깃 문장에 대한 값인 $M$의 값을 참조한다. 키와 밸류 행렬은 입력 문장의 표현을 가져서 $R$의 값을 참조한다. 이때 장점은 무엇일까? 셀프 어텐션을 단계적으로 계산하면서 좀 더 자세히 알아보자.</p><p>셀프 어텐션의 첫 번째 단계는 쿼리, 키 행렬 간의 내적을 계산하는 것이다. 앞에서 설명했듯이 쿼리 행렬은 $M$의 값을, 키 행렬은 $R$의 값을 참조했다. 쿼리 , 키 행렬값은 다음 그림과 같다.</p><p> <img data-src="/assets/img/transformer/decoder/qkT.png" alt="" data-proofer-ignore></p><p>위 행렬 $Q_i\cdot K_i^T$를 통해 다음 사실을 이해할 수 있다.</p><ul><li>행렬의 첫 번째 행에서 쿼리 벡터 $q_1(<code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code>)$와 모든 키 벡터 $k_1(I), k_2(am), k_3(good)$ 사이의 내적을 계산한다. 첫 행은 타깃 단어 $<code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code>$가 입력 문장의 모든 단어 (I, am, good)와 얼마나 유사한지를 계산하는 것으로 해석할 수 있다.</ul><p>멀티 헤드 어텐션의 다음 단계는 $Q_i\cdot K_i^T$를 $\sqrt{d_k}$로 나누는 것이다. 이후 소프트맥스 함수를 적용하면 스코어 행렬을 얻을 수 있다.</p>\[Z_i=softmax({Q_iK_i^T\over \sqrt{d_k}})V_i\]<p>어텐션 행렬은 다음 그림처럼 표현할 수 있다.</p><p> <img data-src="/assets/img/transformer/decoder/score_i.png" alt="" data-proofer-ignore></p><p>타깃 문장의 어텐션 행렬 $Z_i$의 경우 각 스코어에 대한 가중치를 반영한 벡터값의 합으로 계산된다. 예를 들어 단어 “Je”, $Z_2$의 셀프 벡터값을 계산한다고 가정하자.</p>\[Z_2=0.98 \cdot V_1(I)+0.02 \cdot V_2(am)+0.0\cdot V_3(good)\]<p>이와 유사하게 $h$개의 헤드에 대해 어텐션 행렬을 구한 후 이를 연결하고, 가중치 행렬 $W_0$을 곱하면 최종 어텐션 행렬을 구할 수 있다.</p>\[multi head \ attention = concatenate(Z_1, Z_2, \cdots, Z_h)W_0\]<h2 id="133-피드포워드-네트워크"><span class="mr-2">1.3.3 피드포워드 네트워크</span><a href="#133-피드포워드-네트워크" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>다음 그림 처럼 디코더의 다음 서브레이어는 feedforward network이다.</p><p> <img data-src="/assets/img/transformer/decoder/decoder_block.png" alt="" data-proofer-ignore></p><p>디코더의 피드포워드 네트워크는 앞에서 배운 인코더의 피드포워드 네트워크와 동일한 구조다. 이제 add와 norm에 대해 알아보자.</p><h2 id="134-add와-norm-요소"><span class="mr-2">1.3.4 add와 norm 요소</span><a href="#134-add와-norm-요소" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>인코더에서 배운 것처럼 add와 norm 구성 요소는 [그림 1-60]처럼 서브레이어의 입력과 출력을 서로 연결한다.</p><p> <img data-src="/assets/img/transformer/decoder/img1-60.png" alt="" data-proofer-ignore> <em>그림 1-60 add와 norm 요소가 있는 디코더 블록</em></p><h2 id="135-선형과-소프트맥스-레이어"><span class="mr-2">1.3.5 선형과 소프트맥스 레이어</span><a href="#135-선형과-소프트맥스-레이어" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>디코더가 타깃 문장에 대한 표현을 학습시키면 [그림 1-61] 처럼 최상위 디코더에서 얻은 출력값을 선형 및 소프트맥스 레이어에 전달한다.</p><p> <img data-src="/assets/img/transformer/decoder/img1-61.png" alt="" data-proofer-ignore> <em>그림 1-61 선형 및 소프트맥스 레이어</em></p><p>선형 레이어의 경우 그 크기가 어휘(vocabulary) (이하 vocab) 크기와 같은 logit 형태이다. vocab이 다음과 같이 3개의 요소로 구성되어 있다고 가정하자.</p>\[vocabulary=[bien, Je, vais]\]<p>선형 레이어가 반환하는 로짓은 크기가 3인 벡터 형태가 된다. 소프트맥스 함수를 사용해 로짓값을 확률값으로 변환한 다음, 디코더에서 가장 높은 확률값을 갖는 인덱스의 단어로 출력한다. 다음 예제를 통해 더 자세히 알아보자.</p><p>디코더의 입력 단어가 $<code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code>$와 $Je$ 라고 할 때 디코더는 입력 단어를 보고 다음 단어를 예측한다. 이를 위해 디코더에서는 최상위 출력값을 가져와서 선형 레이어에 입력한다. 이 선형 레이어에서 vocab 크기와 동일한 크기의 로짓 벡터를 생성한다. 이 로짓값이 다음과 같다고 가정하자.</p>\[logit=[45,40,49]\]<p>이 로짓값에 소프트맥스 함수를 적용하고 확률값 $prob$을 얻는다.</p>\[prob=[0.0179, 0.000, 0.981]\]<p>앞의 행렬에서 인덱스가 2인 경우 확률값은 $0.981$로 가장 높다. 따라서 vocab엣서 인덱스가 2인 $vais$가 타깃 문장의 다음 단어로 예측된다. 이런 방식으로 디코더는 타깃 문장의 다음 단어를 예측한다.</p><h2 id="136-디코더-모든-구성-요소-연결하기"><span class="mr-2">1.3.6 디코더 모든 구성 요소 연결하기</span><a href="#136-디코더-모든-구성-요소-연결하기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>다음 그림은 디코더가 2개 쌓인 형태다. 간결하게 표현하기 위해 디코더의 첫 번째 부분만 확장해서 표현했다.</p><p> <img data-src="/assets/img/transformer/decoder/img1-62.png" alt="" data-proofer-ignore> <em>그림 1-62 디코더 2개가 쌓인 형태</em></p><p>[그림 1-62]에서 다음 사실을 알 수 있다.</p><ol><li>먼저 디코더에 대한 입력 문장을 임베딩 행렬로 변환한 다음 위치 인코딩 정보를 추가하고 디코더(디코더 1)에 입력한다.<li>디코더는 입력을 가져와서 마스크된 멀티 헤드 어텐션 레이어에 보내고, 출력으로 어텐션 행렬 $M$을 반환한다.<li>어텐션 행렬 $M$, 인코딩 표현 $R$을 입력받아 멀티 헤드 어텐션 레이어 (인코더-디코더 어텐션 레이어)에 값을 입력하고, 출력으로 새로운 어텐션 행렬을 생성한다.<li>인코더-디코더 어텐션 레이어에서 출력한 어텐션 행렬을 다음 서브레이어인 피드포워드 네트워크에 입력한다. 피드포워드 네트워크에서는 이 입력값을 받아서 디코더의 표현으로 값을 출력한다.<li>디코더 1의 출력값을 디코더 2의 입력값으로 사용한다.<li>디코더2는 디코더 1에서 수행한 프로세스와 동일한 형태를 진행하고, 타깃 문장에 대한 디코더 표현을 반환한다.</ol><p>디코더의 경우 $N$개의 디코더를 쌓을 수 있다. 이때 최종 디코더 (최상위 디코더)에서 얻은 출력 (디코더 표현)은 타깃 문장의 표현이 된다. 다음으로 타깃 문장의 디코더 표현을 선형 및 소프트맥스 레이어에 입력하고 최종으로 예측된 단어를 얻는다.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/nlp/'>NLP</a>, <a href='/categories/transformer/'>TRANSFORMER</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >nlp</a> <a href="/tags/transformer/" class="post-tag no-text-decoration" >transformer</a> <a href="/tags/decoder/" class="post-tag no-text-decoration" >decoder</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%94%94%EC%BD%94%EB%8D%94+-+MNG&url=https%3A%2F%2Fmoongthy.github.io%2Fposts%2Ftransformer-decoder%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%94%94%EC%BD%94%EB%8D%94+-+MNG&u=https%3A%2F%2Fmoongthy.github.io%2Fposts%2Ftransformer-decoder%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fmoongthy.github.io%2Fposts%2Ftransformer-decoder%2F&text=%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%94%94%EC%BD%94%EB%8D%94+-+MNG" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script src="https://utteranc.es/client.js" repo="Moongthy/Moongthy.github.io" issue-term="pathname" theme="github-dark" crossorigin="anonymous" async> </script></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/fgsm/">적대적 예제의 설명과 활용 (Fast Gradient Sign Method, FGSM) - Ian J.Goodfellow, Jonathan Shelens & Christian Szegedy</a><li><a href="/posts/pgd/">적대적 공격에 저항하는 딥러닝 모델을 향하여 (Towards Deep Learning Models Resistant to Adversarial Attacks - Aleksnader Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu)</a><li><a href="/posts/dtoverview/">1 결정 트리 개요</a><li><a href="/posts/dtalg/">2 결정 트리 알고리즘</a><li><a href="/posts/varbias/">3 분산(variance)과 편향(bias)</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/decision-tree/">decision tree</a> <a class="post-tag" href="/tags/kaggle/">kaggle</a> <a class="post-tag" href="/tags/bert/">bert</a> <a class="post-tag" href="/tags/gradient-boosting/">gradient boosting</a> <a class="post-tag" href="/tags/random-forest/">random forest</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/adversarial-example/">adversarial example</a> <a class="post-tag" href="/tags/decoder/">decoder</a> <a class="post-tag" href="/tags/distillbert/">distillbert</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/transformer-endecoder/"><div class="card-body"> <em class="small" data-ts="1662692400" data-df="ll" > Sep 9, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>트랜스포머-인코더 디코더 결합</h3><div class="text-muted small"><p> 인코더와 디코더를 포함한 완전한 모양의 트랜스포머 아키텍처는 다음과 같다. 그림 1-63 트랜스포머 인코더, 디코더 $N \times$ 는 인코더와 디코더를 $N$개 쌓을 수 있음으로 나타낸다. [그림 1-63]에서 알 수 있듯이, 입력 문장 (소스 문장)을 입력하면 인코더에서는 해당 문장에 대한 표현을 학습시키고, 그 결괏값을 디코더...</p></div></div></a></div><div class="card"> <a href="/posts/transformer-encoder/"><div class="card-body"> <em class="small" data-ts="1662687000" data-df="ll" > Sep 9, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>트랜스포머-인코더</h3><div class="text-muted small"><p> 트랜스포머는 자연어 처리에서 주로 사용하는 딥러닝 아키텍처 중 하나다. 트랜스포머가 추현한 뒤로, 다양한 태스크에 활용되었던 순환신경망(RNN)과 장단기 메모리(LSTM)는 트랜스포머로 대체된다. BERT, GPT, T5 등과 같은 다양한 자연어 처리(NLP)모델에 트랜스포머 아키텍처가 적용됐다. 이번 장에서는 트랜스포머의 기본적인 의미부터 이해해볼...</p></div></div></a></div><div class="card"> <a href="/posts/bert-basic/"><div class="card-body"> <em class="small" data-ts="1662693000" data-df="ll" > Sep 9, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>BERT의 기본 개념</h3><div class="text-muted small"><p> BERT (Bidirectional Encoder Representation from Transformer)는 구글에서 발표한 최신 임베딩 모델이다. 질문에 대한 대답, 텍스트 생성, 문장 분류 등과 같은 태스크에서 가장 좋은 성능을 도출해 자연어 처리 분야에 크게 기여했다. BERT가 성공한 주된 이유는 문맥이 없는 워드투벡터와 같은 다른 인기 있는...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/transformer-encoder/" class="btn btn-outline-primary" prompt="Older"><p>트랜스포머-인코더</p></a> <a href="/posts/transformer-endecoder/" class="btn btn-outline-primary" prompt="Newer"><p>트랜스포머-인코더 디코더 결합</p></a></div><script src="https://utteranc.es/client.js" repo="" issue-term="" crossorigin="anonymous" async> </script> <script type="text/javascript"> $(function() { const origin = "https://utteranc.es"; const iframe = "iframe.utterances-frame"; const lightTheme = "github-light"; const darkTheme = "github-dark"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } addEventListener("message", (event) => { let theme; /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */ if (event.origin === origin) { /* page initial */ theme = initTheme; } else if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); } else { return; } const message = { type: "set-theme", theme: theme }; const utterances = document.querySelector(iframe).contentWindow; utterances.postMessage(message, origin); }); }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/username">mng</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/decision-tree/">decision tree</a> <a class="post-tag" href="/tags/kaggle/">kaggle</a> <a class="post-tag" href="/tags/bert/">bert</a> <a class="post-tag" href="/tags/gradient-boosting/">gradient boosting</a> <a class="post-tag" href="/tags/random-forest/">random forest</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/adversarial-example/">adversarial example</a> <a class="post-tag" href="/tags/decoder/">decoder</a> <a class="post-tag" href="/tags/distillbert/">distillbert</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
