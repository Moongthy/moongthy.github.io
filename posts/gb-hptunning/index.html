<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="3 그레디언트 부스팅 하이퍼파라미터 튜닝" /><meta name="author" content="mng" /><meta property="og:locale" content="en" /><meta name="description" content="이번에는 가장 중요한 그레디언트 부스팅의 매개변수인 learning_rate과 모델의 트리 개수 또는 반복인 n_estimators에 초점을 맞출 것이다. 또한 확률적 그레디언트 부스팅(stochastic gradient boosting) 을 만드는 subsample 매개변수도 알아볼 것이다. 그리고 RandomizedSearchCV를 통해 XGBoost와 결과를 비교해보겠다." /><meta property="og:description" content="이번에는 가장 중요한 그레디언트 부스팅의 매개변수인 learning_rate과 모델의 트리 개수 또는 반복인 n_estimators에 초점을 맞출 것이다. 또한 확률적 그레디언트 부스팅(stochastic gradient boosting) 을 만드는 subsample 매개변수도 알아볼 것이다. 그리고 RandomizedSearchCV를 통해 XGBoost와 결과를 비교해보겠다." /><link rel="canonical" href="https://moongthy.github.io/posts/gb-hptunning/" /><meta property="og:url" content="https://moongthy.github.io/posts/gb-hptunning/" /><meta property="og:site_name" content="MNG" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-06-02T19:02:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="3 그레디언트 부스팅 하이퍼파라미터 튜닝" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@mng" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"mng"},"dateModified":"2022-06-02T23:13:50+09:00","datePublished":"2022-06-02T19:02:00+09:00","description":"이번에는 가장 중요한 그레디언트 부스팅의 매개변수인 learning_rate과 모델의 트리 개수 또는 반복인 n_estimators에 초점을 맞출 것이다. 또한 확률적 그레디언트 부스팅(stochastic gradient boosting) 을 만드는 subsample 매개변수도 알아볼 것이다. 그리고 RandomizedSearchCV를 통해 XGBoost와 결과를 비교해보겠다.","headline":"3 그레디언트 부스팅 하이퍼파라미터 튜닝","mainEntityOfPage":{"@type":"WebPage","@id":"https://moongthy.github.io/posts/gb-hptunning/"},"url":"https://moongthy.github.io/posts/gb-hptunning/"}</script><title>3 그레디언트 부스팅 하이퍼파라미터 튜닝 | MNG</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="MNG"><meta name="application-name" content="MNG"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/archan.gif" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">MNG</a></div><div class="site-subtitle font-italic">Student, 26</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/Moongthy" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['mungeunj','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>3 그레디언트 부스팅 하이퍼파라미터 튜닝</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>3 그레디언트 부스팅 하이퍼파라미터 튜닝</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1654164120" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jun 2, 2022 </em> </span> <span> Updated <em class="" data-ts="1654179230" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jun 2, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> mng </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1788 words"> <em>9 min</em> read</span></div></div></div><div class="post-content"><p>이번에는 가장 중요한 그레디언트 부스팅의 매개변수인 learning_rate과 모델의 트리 개수 또는 반복인 n_estimators에 초점을 맞출 것이다. 또한 <span style="color:red"> 확률적 그레디언트 부스팅(stochastic gradient boosting) </span> 을 만드는 subsample 매개변수도 알아볼 것이다. 그리고 RandomizedSearchCV를 통해 XGBoost와 결과를 비교해보겠다.</p><h2 id="31-learning_rate"><span class="mr-2">3.1 Learning_rate</span><a href="#31-learning_rate" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>이전에 GradientBoostingRegressor의 learning_rate 매개변수 값을 1.0에서 사이킷런 기본값인 0.1로 바꾸어서 성능을 크게 높였었다.</p><p>learning_rate은 모델 구축에 너무 큰 영향을 끼치지 않도록 개별 트리의 기여를 줄인다. 이를 <span style="color:red"> 축소(shrinkage) </span> 라고도 부른다. 이 매개변수를 주의 깊게 조정하지 않고 기본 학습기의 오차를 기반으로 전체 앙상블을 만들면 모델에 처음 추가된 트리의 영향이 너무 크게 된다.</p><p>learning_rate은 개별 트리의 영향을 제한한다. 일반적으로 트리의 개수인 n_estimators를 늘리면 learning_rate은 줄여야 한다.</p><p>최적의 learning_rate 값을 결정하는 것은 n_estimators에 따라 다르다. 먼저 n_estimators를 고정하고 learning_rate의 효과를 확인해보자. learning_rate을 0에서 1까지 바꾸어보자. learning_rate=1이면 트리 결과에 어떠한 조정도 하지 않는다는 의미이다. 기본값 0.1은 트리의 영향을 10%로 줄인다는 말이다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="n">learning_rate_values</span> <span class="o">=</span> <span class="p">[.</span><span class="mi">001</span><span class="p">,</span> <span class="p">.</span><span class="mi">01</span><span class="p">,</span> <span class="p">.</span><span class="mi">05</span><span class="p">,</span> <span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="p">.</span><span class="mi">15</span><span class="p">,</span> <span class="p">.</span><span class="mi">2</span><span class="p">,</span> <span class="p">.</span><span class="mi">3</span><span class="p">,</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>

<span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">learning_rate_values</span><span class="p">:</span>
  <span class="n">gbr</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
                                  <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>
  <span class="n">gbr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">gbr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
  <span class="n">rmse</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="p">.</span><span class="mi">5</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'lr: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s">, RMSE: </span><span class="si">{</span><span class="n">rmse</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></table></code></div></div><p>lr: 0.001, RMSE: 1633.0261400367258 lr: 0.01, RMSE: 831.5430182728547 lr: 0.05, RMSE: 685.0192988749717 lr: 0.1, RMSE: 653.7456840231495 lr: 0.15, RMSE: 687.666134269379 lr: 0.2, RMSE: 664.312804425697 lr: 0.3, RMSE: 689.4190385930236 lr: 0.5, RMSE: 693.8856905068778 lr: 1.0, RMSE: 936.3617413678853</p><p>출력에서 볼 수 있듯이 기본 learning_rate 값 0.1이 300개의 트리에서 가장 좋은 성능을 낸다.</p><p>이번에는 n_estimators를 바꾸어 보자. 위의 코드에서 n_estimators를 30, 300, 3000으로 바꾸고 learning_rate에 대한 RMSE 그래프를 그리면 다음과 같다.</p><p><img data-src="/assets/img/gb-hptunning/fig30.png" data-proofer-ignore></p><p>결과에서 보듯이 30개 트리를 사용하는 경우 learning_rate=0.3 근처일 때 최상의 성능을 낸다.</p><p>300개의 트리를 사용할 때 그래프를 그려보자.</p><p><img data-src="/assets/img/gb-hptunning/fig300.png" data-proofer-ignore></p><p>learning_rate=0.1일 때 최소임이 잘 나타난다.</p><p>이제 3000개의 트리를 사용하는 경우 learning_rate 그래프를 그려보자.</p><p><img data-src="/assets/img/gb-hptunning/fig3000.png" data-proofer-ignore></p><p>3000개 트리를 사용하는 경우 learning_rate이 0.01일 때 최상의 점수를 낸다.</p><p>이 그래프들은 <span style="color:red"> learning_rate과 n_estimator 매개변수를 함께 튜닝 </span> 해야 한다는 점을 알려준다.</p><h2 id="32-기본-학습기"><span class="mr-2">3.2 기본 학습기</span><a href="#32-기본-학습기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>그레디언트 부스팅의 회귀 모델의 기본학습기는 <span style="color:red"> 결정트리 </span> 이다. 이 결정 트리를 미세 튜닝할 필요는 없지만 정확도를 높이기 위해 매개변수를 조정 할 수 있다.</p><p>예를 들어, 다음처럼 max_depth를 1,2,3,4로 바꾸면서 결과를 비교해 볼 수 있다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="n">depths</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depths</span><span class="p">:</span>
  <span class="n">gbr</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
                                  <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">gbr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">gbr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
  <span class="n">rmse</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="p">.</span><span class="mi">5</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'max depth: </span><span class="si">{</span><span class="n">depth</span><span class="si">}</span><span class="s">, RMSE: </span><span class="si">{</span><span class="n">rmse</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></table></code></div></div><p>max depth: None, RMSE: 869.2788645118395 max depth: 1, RMSE: 707.8261886858736 max depth: 2, RMSE: 653.7456840231495 max depth: 3, RMSE: 646.4045923317708 max depth: 4, RMSE: 663.048387855927</p><p>max_depth=3일 때 최상의 결과를 낸다.</p><h2 id="33-subsample"><span class="mr-2">3.3 Subsample</span><a href="#33-subsample" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>subsample 매개변수는 기본 학습기에 사용될 샘플의 비율을 지정한다. subsample을 기본값인 1.0보다 작게 설정하면 트리를 훈련할 때 샘플의 일부만 사용하게 된다. 예를 들어 subsample=0.8인 경우 80%의 훈련 세트만 사용하여 각 트리를 훈련한다.</p><p>max_depth=3으로 지정하고 subsample에 따라 점수 변화를 확인해보자.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">.</span><span class="mi">9</span><span class="p">,</span> <span class="p">.</span><span class="mi">8</span><span class="p">,</span> <span class="p">.</span><span class="mi">7</span><span class="p">,</span> <span class="p">.</span><span class="mi">6</span><span class="p">,</span> <span class="p">.</span><span class="mi">5</span><span class="p">]</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
  <span class="n">gbr</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
                                  <span class="n">subsample</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">gbr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">gbr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
  <span class="n">rmse</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="p">.</span><span class="mi">5</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'subsample: </span><span class="si">{</span><span class="n">sample</span><span class="si">}</span><span class="s">, RMSE: </span><span class="si">{</span><span class="n">rmse</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></table></code></div></div><p>subsample: 1, RMSE: 646.4045923317708 subsample: 0.9, RMSE: 620.1819001443569 subsample: 0.8, RMSE: 617.2355650565677 subsample: 0.7, RMSE: 612.9879156983139 subsample: 0.6, RMSE: 622.6385116402317 subsample: 0.5, RMSE: 626.9974073227554</p><p>300개 트리, 최대깊이 3일 때 subsample=.7에서 가장 좋은 점수를 냈다.</p><p>subsample이 1보다 작을 때 이런 모델을 <span style="color:red"> 확률적 그레디언트 부스팅 </span> 이라고 부른다. 확률적이라는 말은 모델에 무작위성이 주입된다는 말이다.</p><h2 id="34-randomizedsearchcv"><span class="mr-2">3.4 RandomizedSearchCV</span><a href="#34-randomizedsearchcv" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>잘 동작하는 모델을 얻었지만 그리드 서치를 수행하지 않았다. 앞선 모델을 참고했을 때 max_depth=3, subsample=.7, n_estimators=300, learning_rate=.1 근처가 그리드 서치로 탐색하기 좋은 출발점이다. n_estimators는 높이고 learning_rate는 낮추는 것이 좋을 것 같다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'subsample'</span><span class="p">:</span> <span class="p">[.</span><span class="mi">65</span><span class="p">,</span> <span class="p">.</span><span class="mi">7</span><span class="p">,</span> <span class="p">.</span><span class="mi">75</span><span class="p">],</span>
    <span class="s">'n_estimators'</span><span class="p">:</span> <span class="p">[</span><span class="mi">300</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span>
    <span class="s">'learning_rate'</span><span class="p">:</span> <span class="p">[.</span><span class="mi">05</span><span class="p">,</span> <span class="p">.</span><span class="mi">075</span><span class="p">,</span> <span class="p">.</span><span class="mi">1</span><span class="p">]</span>
<span class="p">}</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<span class="n">gbr</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">rand_reg</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">gbr</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                              <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_mean_squared_error'</span><span class="p">,</span>
                              <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">rand_reg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">rand_reg</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="n">best_params</span> <span class="o">=</span> <span class="n">rand_reg</span><span class="p">.</span><span class="n">best_params_</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'best params: </span><span class="si">{</span><span class="n">best_params</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="n">best_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="n">rand_reg</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'train score: </span><span class="si">{</span><span class="n">best_score</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">rmse_test</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="p">.</span><span class="mi">5</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'test score: </span><span class="si">{</span><span class="n">rmse_test</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></table></code></div></div><p>best params: {‘subsample’: 0.65, ‘n_estimators’: 300, ‘learning_rate’: 0.05} train score: 636.200 test score: 625.985</p><p>이 매개변수에서 한개 씩 혹은 여러 개를 바꿔서 실험해볼 수 있다. n_estimators=300이 최상의 모델이지만 learning_rate를 조정하고 n_estimators를 증가시켜 더 좋은 결과를 얻을 수 있다. subsamples도 실험해볼 수 있다.</p><p>몇 번의 실험을 반복한 후에 다음 결과를 얻었다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="n">gbr</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1600</span><span class="p">,</span>
                                <span class="n">subsample</span><span class="o">=</span><span class="p">.</span><span class="mi">75</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="p">.</span><span class="mi">02</span><span class="p">,</span>
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">gbr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">gbr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="p">.</span><span class="mi">5</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'RMSE: </span><span class="si">{</span><span class="n">rmse</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></table></code></div></div><p>RMSE: 596.9544588974487</p><p>n_estimators를 1600으로 크게 늘리고, learning_rate를 .02로 줄였다. 그 다음 이전과 비슷한 subsample=.75와 max_depth=3으로 하여 597의 RMSE를 얻었다.</p><p>이제 XGBoost가 위에 언급했던 매개변수에서 그레디언트 부스팅과 어떻게 다른지 확인해보자.</p><h2 id="35-xgboost"><span class="mr-2">3.5 XGBoost</span><a href="#35-xgboost" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>XGBoost의 일반적인 구조는 동일한 그레디언트 부스팅의 고급버전이다. 즉 잔차로부터 훈련한 트리를 추가하여 약한 학습기를 강력한 학습기로 바꾼다.</p><p>이전에 소개한 매개변수와 다른 것은 learning_rate으로 XGBoost에서는 eta이다.</p><p>동일한 매개변수로 XGBoost 회귀 모델을 만들어보고 결과를 비교해보자.</p><p>다음처럼 xgboost 패키지에서 XGBRegressor를 임포트하고, 모델을 초기화하고 훈련한 다음 점수를 계산한다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBRegressor</span>
<span class="n">xg_reg</span> <span class="o">=</span> <span class="n">XGBRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1600</span><span class="p">,</span>
                      <span class="n">subsample</span><span class="o">=</span><span class="p">.</span><span class="mi">75</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="p">.</span><span class="mi">02</span><span class="p">,</span>
                      <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">xg_reg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">xg_reg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">MSE</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="p">.</span><span class="mi">5</span><span class="p">)</span>
</pre></table></code></div></div><p>584.3395337495713</p><p>점수가 더 좋다. 더 좋은 점수가 나온 이유는 다음에 자세히 알아볼 것이다.</p><p>머신러닝 모델을 만들 때 성능과 속도는 가장 중요한 두가지 요소이다. XGBoost가 매우 성능이 높다는 것을 여러 번 보았다. XGBoost가 일반적으로 그레디언트 부스팅보다 선호되는 이유는 더 좋은 성능을 내고 더 빠르기 때문이다.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/xgb-for-gradient-boosting/'>XGB FOR GRADIENT BOOSTING</a>, <a href='/categories/gradient-boosting/'>GRADIENT BOOSTING</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/gradient-boosting/" class="post-tag no-text-decoration" >gradient boosting</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=3+%EA%B7%B8%EB%A0%88%EB%94%94%EC%96%B8%ED%8A%B8+%EB%B6%80%EC%8A%A4%ED%8C%85+%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0+%ED%8A%9C%EB%8B%9D+-+MNG&url=https%3A%2F%2Fmoongthy.github.io%2Fposts%2Fgb-hptunning%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=3+%EA%B7%B8%EB%A0%88%EB%94%94%EC%96%B8%ED%8A%B8+%EB%B6%80%EC%8A%A4%ED%8C%85+%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0+%ED%8A%9C%EB%8B%9D+-+MNG&u=https%3A%2F%2Fmoongthy.github.io%2Fposts%2Fgb-hptunning%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fmoongthy.github.io%2Fposts%2Fgb-hptunning%2F&text=3+%EA%B7%B8%EB%A0%88%EB%94%94%EC%96%B8%ED%8A%B8+%EB%B6%80%EC%8A%A4%ED%8C%85+%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0+%ED%8A%9C%EB%8B%9D+-+MNG" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div><script src="https://utteranc.es/client.js" repo="Moongthy/Moongthy.github.io" issue-term="pathname" theme="github-dark" crossorigin="anonymous" async> </script></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/fgsm/">적대적 예제의 설명과 활용 (Fast Gradient Sign Method, FGSM) - Ian J.Goodfellow, Jonathan Shelens & Christian Szegedy</a><li><a href="/posts/pgd/">적대적 공격에 저항하는 딥러닝 모델을 향하여 (Towards Deep Learning Models Resistant to Adversarial Attacks - Aleksnader Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu)</a><li><a href="/posts/dtoverview/">1 결정 트리 개요</a><li><a href="/posts/dtalg/">2 결정 트리 알고리즘</a><li><a href="/posts/varbias/">3 분산(variance)과 편향(bias)</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/decision-tree/">decision tree</a> <a class="post-tag" href="/tags/kaggle/">kaggle</a> <a class="post-tag" href="/tags/bert/">bert</a> <a class="post-tag" href="/tags/gradient-boosting/">gradient boosting</a> <a class="post-tag" href="/tags/random-forest/">random forest</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/adversarial-example/">adversarial example</a> <a class="post-tag" href="/tags/decoder/">decoder</a> <a class="post-tag" href="/tags/distillbert/">distillbert</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/bagtoboost/"><div class="card-body"> <em class="small" data-ts="1654164000" data-df="ll" > Jun 2, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>1 배깅에서 부스팅까지</h3><div class="text-muted small"><p> 랜덤 포레스트 같은 앙상블 머신러닝 알고리즘이 많은 모델을 하나로 연결하여 더 나은 예측을 만드는 이유를 알고있다. 랜덤 포레스트는 (결정 트리에서) 부트 스트랩 샘플 을 사용하기 때문에 배깅 알고리즘으로 분류된다. 이와 달리 부스팅 은 개별 트리의 실수로부터 학습한다. 이전 트리의 오차를 기반으로 새로운 트리를 훈련하는 것이 기본적인 아...</p></div></div></a></div><div class="card"> <a href="/posts/make-gbmodel/"><div class="card-body"> <em class="small" data-ts="1654164060" data-df="ll" > Jun 2, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>2 그레디언트 부스팅 구현</h3><div class="text-muted small"><p> 그레디언트 부스팅의 작동 방식을 살펴보고 이전 트리의 오차에 새로운 트리를 훈련하는 식으로 그레디언트 부스팅 모델을 만들어본다. 여기서 수학적인 핵심 요소는 잔차(residual)이다. 그 다음 사이킷-런의 그레디언트 부스팅 모델을 사용해 동일한 결과를 구해볼 것이다. 2.1 잔차 (Residual) 잔차는 타깃과 모델의 예측 사이의 차이이다. 통...</p></div></div></a></div><div class="card"> <a href="/posts/gb-vs-xgb/"><div class="card-body"> <em class="small" data-ts="1654164180" data-df="ll" > Jun 2, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>4 빅 데이터 - 그레디언트 부스팅 vs XGBoost</h3><div class="text-muted small"><p> 현실 세계의 데이터셋은 매우 거대하며 수조 개의 데이터 포인트로 이루어질 수 있다. 컴퓨터 한 대의 자원은 제약되어 있기 때문에 한 대의 컴퓨터로만 작업하는 것은 단점이 될 수 있다. 빅 데이터를 다룰 때 종종 병렬 컴퓨팅으 활용하려고 클라우드를 사용한다. 대용량 데이터셋은 계산의 한계를 넘어설 때가 있다. 지금까지 사용한 데이터셋은 수만 개의 행과...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/make-gbmodel/" class="btn btn-outline-primary" prompt="Older"><p>2 그레디언트 부스팅 구현</p></a> <a href="/posts/gb-vs-xgb/" class="btn btn-outline-primary" prompt="Newer"><p>4 빅 데이터 - 그레디언트 부스팅 vs XGBoost</p></a></div><script src="https://utteranc.es/client.js" repo="" issue-term="" crossorigin="anonymous" async> </script> <script type="text/javascript"> $(function() { const origin = "https://utteranc.es"; const iframe = "iframe.utterances-frame"; const lightTheme = "github-light"; const darkTheme = "github-dark"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } addEventListener("message", (event) => { let theme; /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */ if (event.origin === origin) { /* page initial */ theme = initTheme; } else if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); } else { return; } const message = { type: "set-theme", theme: theme }; const utterances = document.querySelector(iframe).contentWindow; utterances.postMessage(message, origin); }); }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/username">mng</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/decision-tree/">decision tree</a> <a class="post-tag" href="/tags/kaggle/">kaggle</a> <a class="post-tag" href="/tags/bert/">bert</a> <a class="post-tag" href="/tags/gradient-boosting/">gradient boosting</a> <a class="post-tag" href="/tags/random-forest/">random forest</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/adversarial-example/">adversarial example</a> <a class="post-tag" href="/tags/decoder/">decoder</a> <a class="post-tag" href="/tags/distillbert/">distillbert</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
