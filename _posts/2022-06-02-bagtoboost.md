---
title: 1 배깅에서 부스팅까지
author: mng
date: 2022-06-02 19:00:00 +0900
categories: [XGB FOR GRADIENT BOOSTING, GRADIENT BOOSTING]
tags: [gradient boosting, bagging]
---

랜덤 포레스트 같은 앙상블 머신러닝 알고리즘이 많은 모델을 하나로 연결하여 더 나은 예측을 만드는 이유를 알고있다.
랜덤 포레스트는 (결정 트리에서)
<span style="color:red">
부트 스트랩 샘플
</span>
을 사용하기 때문에
<span style="color:red">
배깅
</span>
알고리즘으로 분류된다.

이와 달리
<span style="color:red">
부스팅
</span>
은 개별 트리의 실수로부터 학습한다.
이전 트리의 오차를 기반으로 새로운 트리를 훈련하는 것이 기본적인 아이디어이다.

부스팅에서 기존 트리에 대한 오차를 수정하는 것은 배깅과 다른 접근 방법이다.
배깅 모델에서는 새로운 트리가 이전 트리에 관심을 주지 않는다.
또한 새로운 트리는 부트스트랩 샘플링을 사용해 처음부터 훈련되며 최종 모델은 모든 개별 트리의 결과를 합친 것이다.
하지만 부스팅에서는 개별 트리가 이전 트리를 기반으로 만들어진다.
독립적으로 트리가 동작하지 않으며 다른 트리 위에 만들어진다.

# 1.1 에이다 부스트

에이다 부스트는 인기 있는 초기 부스팅 모델 중 하나이다.
에이다 부스트에서는 새로운 트리가 이전 트리의 오차를 기반으로 가중치를 조정한다.
오류 샘플의 가중치를 높여 잘못된 예측에 더 많은 주의를 기울인다.
에이다 부스트는 이렇게 실수에서 학습하기 때문에 약한 학습기를 강력한 학습기로 만들 수 있다.
약한 학습기는 우연보다 조금 나은 성능을 내는 머신러닝 모델을 말한다.
강한 학습기는 많은 양의 데이터에서 학습하여 매우 잘 수행되는 모델이다.

<span style="color:red">
약한 학습기
</span>
를 강력한 학습기로 변환하는 것이 부스팅 알고리즘의 일반적인 아이디어이다.
약한 학습기는 무작위 예측보다 조금 낫다.
하지만 약한 학습기로 시작하는 데는 목적이 있다.
일반적으로 부스팅은 강력한 기반 모델을 만드는 것이 아니라 반복적으로 오류를 고치는 데 초점을 둔다.
기반 모델이 너무 강력하면 학습 과정이 제한되어 부스팅 모델의 전략을 약화시킨다.

수백번의 반복을 통해 약한 학습기가 강력한 학습기로 바뀐다.
즉, 작은 성능 개선을 오래 지속한다.
부스팅은 지난 수십년 동안 최적의 결과를 만드는 점에서 가장 뛰어난 머신러닝 전략 중 하나이다.

에이다 부스트에 대한 자세한 설명은 생략한다.

이제 성능면에서 약간 더 뛰어난 에이다 부스트의 강력한 대안인 그레디언트 부스팅을 알아보자.

# 1.2 그레디언트 부스팅 (gradient boosting) 의 특징

그레디언트 부스팅은 에이다 부스트와는 다른 전략을 사용한다.
그레디언트 부스팅도 잘못된 예측을 기반으로 조정되지만 한 단계 더 나아간다.
그레디언트 부스팅은 이전 트리의 예측 오차를 기반으로 완전히 새로운 트리를 훈련한다.
즉, 그레디언트 부스팅은 각 트리의 실수를 살펴보고 이런 실수에 대한 완전한 새로운 트리를 만든다.
새로운 트리는 이전 트리에서 올바르게 예측된 값에는 영향을 받지 않는다.

오차에만 초점을 맞추는 머신러닝 알고리즘을 만드려면 정확한 최종 예측을 만들기 위해 오차를 계산하는 방벙이 필요하다.
이런 방법은 모델의 예측과 실제 값 사이의 차이인
<span style="color:red">
잔차 (residual)
</span>
를 활용한다.
일반적인 방법은 다음과 같다.

그레디언트 부스팅은 각 트리 예측값을 더해 모델 평가에 사용한다.

이 아이디어는 그레디언트 부스팅의 고급 버전인 XGBoost의 핵심이므로 트리의 예측을 계산하고 더하는 것을 이해하는 것이 중요하다.
그레디언트 부스팅 모델을 직접 만들어 보면 예측을 계산하고 더하는 과정을 잘 볼 수 있다.
다음 절에서 직접 그레디언트 부스팅 모델을 만들어 볼 것이다.
먼저 그레디언트 부스팅의 작동 방식에 대해 배워보자.
